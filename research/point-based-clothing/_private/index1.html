<!doctype html>
<html lang="en">

<head>
  <link rel="stylesheet" href="saic-project-styles.css">
    
  <script src="jquery.js"></script> 
  <script> 
    $(function(){
      $("#includedContent").load("authors.html"); 
    });
  </script> 
  
  <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
  <!-- jQuery setup -->
  <link rel="stylesheet" href="//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
  <link rel="stylesheet" href="/resources/demos/style.css">
  <script src="https://code.jquery.com/jquery-1.12.4.js"></script>
  <script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script>

  <title>Point-Based Modeling of Human Clothing</title>
  
  
</head>

<body>
<!--   <script src="zepto.min.js"></script>
  <script>
    window.addEventListener('message', function (event) {
      if (height = event.data['height']) {
        $('iframe').css('height', height + 'px')
      }
    });
    // var resizeEvent = new Event('resize');
    // window.dispatchEvent(resizeEvent);
  </script> -->
    
  <div class="container">

   <!-- ====================================================== -->
   <!-- ===================== TITLE ========================== -->
   <!-- ====================================================== -->
   <center>
     <div class="title">Point-Based Modeling of Human Clothing</div>
     <div class="conf">ICCV 2021</div>
   </center>
      
   <p></p>
      
<!--    <div data-include="authors"></div> -->
    <div id="includedContent"></div>

    <!-- ====================================================== -->
    <!-- ===================== TL;DR ======================= -->
    <!-- ====================================================== -->

    <div class="row section">
      <p></p>
      <!-- <big><b>TL;DR:</b> Given a set of RGB(D) images (of a room, person, or any other object) and a sparse point cloud reconstructed from them, our neural network can render this point cloud from any viewpoint. The rendered images look photorealistic even from extreme angles, point cloud representations yield high detailization even for thin object parts, and the system works in real-time. </big> -->
      <big><b>TL;DR:</b> We present a new point-based approach for 3D clothing modeling. We train a draping network based on cloud transformer and get low-dimensional latent space of garment style embeddings - outfit codes. With these we are able to reconstruct clothing geometry (point cloud) given a single image, as well as clothing appearance given a video using neural point-based graphics.</big>
      <p></p>
    </div>

    <!-- ====================================================== -->
    <!-- ===================== LINKS ========================== -->
    <!-- ====================================================== -->
    <center>
      <div class="row section" style="margin-bottom: -12px">
        <div class="col-sm-12">
          <!-- <div class="col-sm-12 center"> -->
          <div class="col-sm-4" style="margin-bottom: 12px">
            <!-- <div class="col-sm-12"> -->
            <a href="https://arxiv.org/pdf/2104.08230.pdf">
              <!-- <img src="assets/projects/pdf.svg" class="img_links img-responsive"> -->
              <svg xmlns="http://www.w3.org/2000/svg" class="img_links img-responsive" viewBox="0 0 24 24"><path d="M11.363 2c4.155 0 2.637 6 2.637 6s6-1.65 6 2.457v11.543h-16v-20h7.363zm.826-2h-10.189v24h20v-14.386c0-2.391-6.648-9.614-9.811-9.614zm4.811 13h-2.628v3.686h.907v-1.472h1.49v-.732h-1.49v-.698h1.721v-.784zm-4.9 0h-1.599v3.686h1.599c.537 0 .961-.181 1.262-.535.555-.658.587-2.034-.062-2.692-.298-.3-.712-.459-1.2-.459zm-.692.783h.496c.473 0 .802.173.915.644.064.267.077.679-.021.948-.128.351-.381.528-.754.528h-.637v-2.12zm-2.74-.783h-1.668v3.686h.907v-1.277h.761c.619 0 1.064-.277 1.224-.763.095-.291.095-.597 0-.885-.16-.484-.606-.761-1.224-.761zm-.761.732h.546c.235 0 .467.028.576.228.067.123.067.366 0 .489-.109.199-.341.227-.576.227h-.546v-.944z"/></svg>
              <p></p>
              <div class="author">Paper</div>
            </a>
            <!-- </div> -->
          </div>
          <div class="col-sm-4" style="margin-bottom: 12px">
            <!-- <div class="col-sm-12"> -->
            <a href="https://youtu.be/SkGfL9i-zr0">
              <svg xmlns="http://www.w3.org/2000/svg" class="img_links img-responsive" viewBox="0 0 24 24"><path d="M10 9.333l5.333 2.662-5.333 2.672v-5.334zm14-4.333v14c0 2.761-2.238 5-5 5h-14c-2.761 0-5-2.239-5-5v-14c0-2.761 2.239-5 5-5h14c2.762 0 5 2.239 5 5zm-4 7c-.02-4.123-.323-5.7-2.923-5.877-2.403-.164-7.754-.163-10.153 0-2.598.177-2.904 1.747-2.924 5.877.02 4.123.323 5.7 2.923 5.877 2.399.163 7.75.164 10.153 0 2.598-.177 2.904-1.747 2.924-5.877z"/></svg>
              <p></p>
              <div class="author">Video</div>
            </a>
            <!-- </div> -->
          </div>
          <div class="col-sm-4" style="margin-bottom: 12px">
            <!-- <div class="col-sm-12"> -->
            <a href="https://github.com/alievk/npbg">
              <!-- <img src="assets/projects/github.svg" class="img_links img-responsive">-->
              <svg xmlns="http://www.w3.org/2000/svg"  class="img_links img-responsive" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
              <p></p>
              <div class="author">Code</div>
            </a>
            <!-- </div> -->
          </div>

          <!-- <div class="col-sm-3" style="margin-bottom: 12px">
            <a href="" >
                <img src="/assets/projects/github.svg" class="img_links img-responsive">
                <div class="author">Video</div>
            </a>
        </div> -->
          <!-- </div> -->
        </div>
      </div>
    </center>

    <!-- ====================================================== -->
    <!-- ===================== ABSTRACT ======================= -->
    <!-- ====================================================== -->
    <div class="row section">
      <!-- <div class="col-xs-8 col-xs-offset-2 text-justify"> -->
      <div class="section_title">Abstract</div>
      <p>We propose a new approach to human clothing modeling based on point clouds. Within this approach, we learn a deep model that can predict point clouds of various outfits, for various human poses, and for various human body shapes. Notably, outfits of various types and topologies can be handled by the same model. Using the learned model, we can infer the geometry of new outfits from as little as a single image, and perform outfit retargeting to new bodies in new poses. We complement our geometric model with appearance modeling that uses the point cloud geometry as a geometric scaffolding and employs neural point-based graphics to capture outfit appearance from videos and to re-render the captured outfits. We validate both geometric modeling and appearance modeling aspects of the proposed approach against recently proposed methods and establish the viability of point-based clothing modeling.</p>
    </div>
      
    <center>
        <iframe width="100%" height="60%" src="https://youtu.be/SkGfL9i-zr0" frameborder="0" allowfullscreen></iframe>
        <iframe width="420" height="315" src="https://www.youtube.com/embed/tgbNymZ7vqY"></iframe>
        <iframe width="846" height="476" src="https://www.youtube.com/embed/9YffrCViTVk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </center>


    <!-- ====================================================== -->
    <!-- ===================== Main idea ======================= -->
    <!-- ====================================================== -->
    <div class="row section">
      <!-- <div class="col-xs-8 col-xs-offset-2 text-justify"> -->
      <div class="section_title">Main idea</div>
      <p>
        Our method could be viewed as 3 parts:
        <p><ol>
          <li>Draping network training and Outfit code space learning</li>
          <li>Single-image outfit geometry reconstruction</li>
          <li>Neural point-based appearance modeling</li>
        </ol></p>
      </p>
    </div>
    
    <div class="row section">
        <div class="subsection_title">Draping network training</div>
        <p>The draping network takes the latent code of a clothing outfit, the subset of vertices of an SMPL body mesh, and predicts the point cloud of the clothing outfit adapted to the body shape and the body pose. We use the recently proposed <a href="https://saic-violet.github.io/cloud-transformers/">Cloud Transformer</a> architecture to perform the mapping.</p>
        <div class="row section"><center>
<!--             <img alt="Random walks in the latent outfit space. We change the Z vector and fix the other input to a certain pose and to a certain body shape." src="assets/images/outfit-code-interpolation.gif" width="50%">  -->
            <figure>
                <img src="assets/images/outfit-code-interpolation.gif" alt="Random walks in the latent outfit space. We change the Z vector and fix the other input to a certain pose and to a certain body shape." width="50%">
                <figcaption>Random walks in the latent outfit space: Z vector changes while the body shape and pose are fixed.</figcaption>
            </figure>
        </center></div>
        <p>We train the model by fitting it to a <a href="https://chalearnlap.cvc.uab.cat/dataset/38/description/">Cloth3D</a> synthetic dataset of physically simulated clothing using <a href="https://arxiv.org/abs/1707.05776">Generative Latent Optimization</a> approach. After fitting, the outfits in the training dataset all get latent codes.</p>
        
<!--         <img name="slide" src=""> -->
    </div>
    
    <div id="slider"></div>
    
    <div class="row section">
        <div class="subsection_title">Single-image outfit geometry reconstruction</div>
        <p>Given an input image, its segmentation, and its SMPL mesh fit, we can find the latent code of the outfit. The code is obtained by optimizing the mismatch between the segmentation and the projection of the point cloud. The draping network remains frozen during this process, only the outfit code vector is being changed ("search in latent space").</p>
        <div class="row section">
            <img src="assets/images/outfit-code-interpolation.gif" width="100%"> 
        </div>
    </div>
    
    <div class="row section">
        <div class="subsection_title">Neural point-based appearance modeling</div>
    </div>

    <!-- ====================================================== -->
    <!-- ======================= VIDEO ======================== -->
    <!-- ====================================================== -->

    <div class="row section">
      <div class="section_title">Video</div>
    </div>

    <!-- allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"  -->
    <!-- <div class="row section videoframe"> -->
    <center>
        <iframe width="100%" height="60%" src="https://youtu.be/SkGfL9i-zr0" 
                frameborder="0" allowfullscreen></iframe>
    </center>
    <!-- </div>-->

  <div class="footer"><center>VISION, LEARNING AND TELEPRESENCE LAB</center></div>
  <div class="footer"><center>SAMSUNG AI CENTER MOSCOW</center></div>

</body>

</html>
